# Developer Notes: How AudioNova Uses AI for Mixing

This page is intended for technical collaborators, contributors, or curious users who want to understand how AudioNova works behind the scenes.

---

## âš™ï¸ How the Mixing Works

At a high level, AudioNovaâ€™s architecture looks like this:

User Prompt â†’ ChatGPT (LLM) â†’ Internal Command Generation â†’ DSP Engine â†’ Audio Output


---

## ğŸ§  Language Model (LLM)

AudioNova uses **ChatGPT or a similar LLM** to interpret natural language prompts. It doesn't hardcode audio instructions â€” it interprets them dynamically.

- Prompts are parsed for:
  - Effects (reverb, compression, EQ)
  - Parameters (e.g. "50% wet", "tight low end")
  - Descriptors (e.g. "gritty", "warm", "club-ready")

- We rely on prompt patterns rather than rigid keyword parsing

- All prompt interpretation is **context-aware** and **generative**, not rule-based

---

## ğŸ› Audio Processing Engine

Once the AI interprets a prompt, the system generates intermediate instructions like:

```json
{
  "effect": "reverb",
  "target": "vocals",
  "amount": 0.25,
  "style": "plate",
  "preDelay": 10
}


These instructions are passed to a DSP backend â€” either custom code, a plugin chain, or a render pipeline â€” depending on the implementation.

ğŸ’¬ Prompt Grammar Evolution:

  Weâ€™re continuously training prompt formats that are:

    - Musician-friendly

    - Conversational

    - Modular (e.g., chaining effects)

  We're designing a prompt grammar that feels like texting your dream engineer â€” not learning a new syntax.

    - Future iterations may use:

    - Structured prompt tagging

    - Reference-based intent matching

    - Few-shot examples per prompt style


ğŸ¤ Contributing:
  
  Interested in helping improve prompt interpretation, grammar design, or backend DSP logic?

  Join the dev team or open an issue. This is mixing by language â€” and weâ€™re just getting started.